# This file is automatically generated by generate_todo.py.
# Files that start with an underscore ("_") have been excluded.

./parsimony/estimators.py:
-------------------------
73: # TODO: Is this a good name?

259: # TODO: Should we use a seed here so that we get deterministic results?

373: # TODO: Should we use a seed here so that we get deterministic results?

489: # TODO: Should we use a seed here so that we get deterministic results?

648: # TODO: Should we use a seed here so that we get deterministic results?

812: # TODO: Should we use a seed here so that we get deterministic results?

1118: # TODO: Should we use a seed here so that we get deterministic results?

1290: # TODO: Should we use a seed here so that we get deterministic results?

./parsimony/utils/utils.py:
--------------------------
21: #TODO: This depends on the OS. We should try to be clever here ...

./parsimony/utils/consts.py:
---------------------------
16: # TODO: MAX_ITER is heavily algorithm-dependent, so we have to think about if
17: # we should include a package-wide maximum at all.

./parsimony/functions/losses.py:
-------------------------------
185: # TODO: Inherit from LinearRegression and add an L2 constraint instead!

362: # TODO: Make the weights sparse.
363: #weights = np.eye(self.X.shape[0])

365: # TODO: Allow the weight vector to be a list.

470: # TODO: Use FastSVD for speedup!

472: self._L = np.max(s) ** 2.0  # TODO: CHECK

632: PWX = 0.5 * np.sqrt(self.weights) * self.X  # TODO: CHECK WITH FOUAD
633: # PW = 0.5 * np.eye(self.X.shape[0]) ## miss np.sqrt(self.W)
634: #PW = 0.5 * np.sqrt(self.W)
635: #PWX = np.dot(PW, self.X)
636: # TODO: Use FastSVD for speedup!

638: self._L = np.max(s) ** 2.0  # TODO: CHECK

643: self._L += self.k  # TODO: CHECK

661: # TODO: Handle mean here?

./parsimony/functions/penalties.py:
----------------------------------
195: # TODO: This should not be able to happen! Do we know it doesn't?

200: # TODO: This should not be able to happen! Do we know it doesn't?

568: # TODO: Check if this is correct!

1116: # TODO: We can share variables between f and df and speed up
1117: # some shared computations.

./parsimony/functions/interfaces.py:
-----------------------------------
106: # TODO: Should all constraints have the projection operator?

328: # TODO: Should L by default take a weight vector as argument?

./parsimony/functions/combinedfunctions.py:
------------------------------------------
35: # TODO: Add penalty_start and mean to all of these!

107: # TODO: We currently only allow one proximal operator. Fix this!

160: # TODO: We currently only allow one proximal operator. Fix this!

628: # TODO: This is not good. Solve this better!

1105: # TODO: This is not a good solution. Can we solve this in a better way?

1157: # TODO: Use max_iter here!!

1207: # TODO: Kernelise this function! See how I did in
1208: # LinearRegressionL1L2TV._beta_hat.

1250: # TODO: Add this function or refactor API!

1333: # TODO: This is not a nice solution. Can we solve it better?

./parsimony/functions/multiblock/losses.py:
------------------------------------------
821: # TODO: Check instead if it is a numpy array.

./parsimony/functions/nesterov/tv.py:
------------------------------------
147: # TODO: This only work if the elements of self._A are scipy.sparse. We
148: # should allow dense matrices as well.

151: # TODO: Instead of p, this should really be the number of non-zero
152: # rows of A.

162: # TODO: Add max_iter here!

./parsimony/functions/nesterov/L1TV.py:
--------------------------------------
62: # WARNING: Number of non-zero rows may differ from p.

149: # TODO: Instead of p, this should really be the number of non-zero
150: # rows of A.

161: # TODO: Add max_iter here!!

./parsimony/functions/nesterov/l1tv.py:
--------------------------------------
62: # WARNING: Number of non-zero rows may differ from p.

149: # TODO: Instead of p, this should really be the number of non-zero
150: # rows of A.

161: # TODO: Add max_iter here!!

./parsimony/functions/nesterov/grouptv.py:
-----------------------------------------
158: # TODO: This only work if the elements of self._A are scipy.sparse. We
159: # should allow dense matrices as well.

165: # TODO: Add max_iter here!

./parsimony/datasets/simulated/grad.py:
--------------------------------------
51: # TODO: Should be RandomUniform(-1, 1) here!

82: # TODO: Should be RandomUniform(-1, 1) here!

./parsimony/datasets/regression/dice5.py:
----------------------------------------
32: # TODO: This is wrong. Shape should be Z, Y, X.

./parsimony/tests/test_logistic_regression.py:
---------------------------------------------
16: # TODO: Test penalty_start.

18: # TODO: Test total variation.

./parsimony/tests/spamsdata.py:
------------------------------
71: # TODO: Don't use print directly.

./parsimony/tests/tests.py:
--------------------------
93: # TODO: Wait for Nose issue #732: https://github.com/nose-devs/nose/issues/732
94: #    @nottest
95: #    def runTest(self):
96: #        """Runs all unit tests.
97: #
98: #        From baseclass "unittest.TestCase".
99: #        """
100: #        RE_TEST = re.compile("[Tt]est[-_]")
101: #        for attr in dir(self):
102: #            if callable(getattr(self, attr)) and RE_TEST.match(attr):
103: #                getattr(self, attr)()

110: # TODO: There is a better way to do this!

./parsimony/tests/test_simulations.py:
-------------------------------------
117: # TODO: Not done. Add more!!

./parsimony/algorithms/explicit.py:
----------------------------------
616: # TODO: Warn if G_new < 0.

775: # TODO: Fix this!

1213: # TODO: We already have f_mid, so we can return a better approximation
1214: # here!

1302: # TODO: Handle the other cases!

1314: # TODO: We seek a root, i.e. where f(x) = 0. The stopping criterion
1315: #       should (could?) thus be abs(f(x)) <= eps!

1334: if abs(x - x_) <= self.eps:  # TODO: Stopping criterion. See above!

1404: # TODO: Investigate what is a good default value here!

1457: # TODO: Investigate what is a good default value here!

1496: # TODO: Does the weights really matter when the function is the
1497: # indicator function?

1531: # TODO: Investigate what is a good default value here!

