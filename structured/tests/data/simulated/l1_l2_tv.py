# -*- coding: utf-8 -*-
"""
Created on Tue Jul 16 12:32:00 2013

@author: Tommy Löfstedt
@email: tommy.loefstedt@cea.fr
"""

__all__ = ['load']

import numpy as np
import structured.utils as utils
import structured.algorithms as algorithms


def load(l, k, gamma, density, snr, P, e):
    """Returns data generated such that we know the exact solution.

    The data generated by this function is fit to the Elastic net + Total
    variation function, i.e.:

        f(b) = (1 / 2).|Xb - y|² + l.|b|_1 + ((1 - l) / 2).|b|² + g.TV(b),

    where |.|_1 is the L1 norm, |.|² is the squared L2 norm and TV is the
    total variation penalty.

    Parameters
    ----------
    l : The L1 regularisation parameter.

    k : The L2 regularisation parameter.

    gamma : The total variation regularisation parameter.

    density : The density of the returned regression vector (fraction of
            non-zero elements). Must be in (0, 1].

    snr : Signal to noise ratio between model and residual.

    P : The matrix to use when building data. This matrix carries the desired
            distribution of the generated data. The generated data will be a
            scaled version of this matrix.

    e : The error vector e = Xb - y. This vector carries the desired
            distribution of the residual. This vector must have unit 2-norm.

    Returns
    -------
    X : The generated X matrix.

    y : The generated y vector.

    beta : The generated regression vector.
    """
    seed = np.random.randint(2147483648)

    low = 0.0
    high = 1.0
    for i in xrange(30):
        print "low:", low, "high:", high
        np.random.seed(seed)
        X, y, beta = _generate(l, k, gamma, density, high, P, e)
        val = np.sqrt(np.sum(np.dot(X, beta) ** 2.0) / np.sum(e ** 2.0))
        if val > snr:
            break
        else:
            low = high
            high = high * 2.0

    def f(x):
        np.random.seed(seed)
        X, y, beta = _generate(l, k, gamma, density, x, P, e)
        return np.sqrt(np.sum(np.dot(X, beta) ** 2.0) / np.sum(e ** 2.0)) - snr

    bm = algorithms.BisectionMethod(max_iter=20)
    bm.run(utils.AnonymousClass(f=f), low, high)

    np.random.seed(seed)
    X, y, beta = _generate(l, k, gamma, density, bm.x, P, e)
    print "snr = %.5f = %.5f = |X.b| / |e| = %.5f / %.5f" \
            % (snr, np.linalg.norm(np.dot(X, beta) / np.linalg.norm(e)),
               np.linalg.norm(np.dot(X, beta)), np.linalg.norm(e))

    return X, y, beta
#    return _generate(l, k, gamma, density, snr, P, e)


def _generate(l, k, gamma, density, snr, P, e):

    l = float(l)
    k = float(k)
    gamma = float(gamma)
    density = float(density)
    snr = float(snr)
    p = P.shape[1]
    ps = int(round(p * density))
    e = e / np.sqrt(np.sum(e ** 2.0))  # Don't trust the user ;-)

    b = np.dot(P.T, e)
    ind = np.flipud(np.argsort(np.abs(b), axis=0))
    b = b[ind[:, 0]]
    #    sign_b = np.sign(b)
    abs_b = np.abs(b)
    P = P[:, ind[:, 0]]

    beta = np.zeros((p, 1))
    for i in xrange(p):
        if i >= ps:
            beta[i, 0] = 0
        else:
            beta[i, 0] = U(0, 1) * snr / np.sqrt(ps)
    beta = np.flipud(np.sort(beta, axis=0))

    X = np.zeros(P.shape)
    a = np.zeros((p, 1))
    a[0, 0] = (-k * beta[0, 0] - l - gamma) / b[0, 0]
    X[:, 0] = P[:, 0] * a[0, 0]
#    print "1 p = %d" % (0)
    for i in xrange(1, ps):
        a[i, 0] = (-k * beta[i, 0] - l) / b[i, 0]
        X[:, i] = P[:, i] * a[i, 0]
#        print "2 p = %d" % (i)
    a[ps, 0] = (-k * beta[ps, 0] - l * U(-1, 1) - gamma * U(-2, 0)) / b[ps, 0]
    X[:, ps] = P[:, ps] * a[ps, 0]
#    print "3 p = %d" % (ps)
    for i in xrange(ps + 1, p - 1):
        a[i, 0] = (-k * beta[i, 0] - l * U(-1, 1) - gamma * U(-2, 2)) / b[i, 0]
        X[:, i] = P[:, i] * a[i, 0]
#        print "4 p = %d" % (i)
    a[p - 1, 0] = (-k * beta[p - 1, 0] - l * U(-1, 1) - gamma * U(-1, 1)) \
                    / b[p - 1, 0]
    X[:, p - 1] = P[:, p - 1] * a[p - 1, 0]
#    print "5 p = %d" % (p - 1)

    y = np.dot(X, beta) - e

    return X, y, beta


def U(a, b):
    t = max(a, b)
    a = float(min(a, b))
    b = float(t)
    return (np.random.rand() * (b - a)) + a